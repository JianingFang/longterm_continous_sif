{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1657cd",
   "metadata": {},
   "source": [
    "## Generate LCSIF clear-inst and clear-daily, MODIS Period\n",
    "### Jianing Fang (jf3423@columbia.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5e4cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import date, time, timedelta\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import LogNorm\n",
    "import astral\n",
    "from astral import sun\n",
    "import skyfield\n",
    "from skyfield.api import load, wgs84\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c4b1e",
   "metadata": {},
   "source": [
    "### Predict OCO-2 Local Overpass time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLE data obtained from https://www.space-track.org/\n",
    "\n",
    "stations_url = '../../data/OCO2_TLE.txt'\n",
    "\n",
    "satellites = load.tle_file(stations_url)\n",
    "ts = load.timescale()\n",
    "print('Loaded', len(satellites), 'satellites')\n",
    "epoches=np.array([sat.epoch.utc_datetime() for sat in satellites])\n",
    "epoch_duration=np.diff(epoches)\n",
    "\n",
    "observe_time=[]\n",
    "lons=[]\n",
    "lats=[]\n",
    "\n",
    "for i, epoch in enumerate(epoches):\n",
    "    if epoch_duration[i].total_seconds() > 1:\n",
    "        epoch_start = ts.from_datetime(epoch)\n",
    "        times=pd.date_range(epoch, periods=int(epoch_duration[i].total_seconds()/10), freq='10S').tolist()\n",
    "        t=ts.from_datetimes(times)\n",
    "        satellite = satellites[i]\n",
    "        geocentric = satellite.at(t)\n",
    "        observe_time.append(t.utc_datetime())\n",
    "        subpoint = wgs84.subpoint(geocentric)\n",
    "        lons.append(subpoint.longitude.degrees)\n",
    "        lats.append(subpoint.latitude.degrees)\n",
    "        \n",
    "lon_array=np.concatenate(lons, axis=0)\n",
    "lat_array=np.concatenate(lats, axis=0)\n",
    "time_array=np.concatenate(observe_time, axis=0)\n",
    "date_array=np.array([t.date() for t in time_array])\n",
    "\n",
    "np.save(\"../../data/processed/OCO2_Track.npy\", np.array([lat_array, lon_array, time_array]).T)\n",
    "\n",
    "date_of_interest=date(2014,9,6)\n",
    "date_sel=date_array == date_of_interest\n",
    "lon_sel=lon_array[date_sel]\n",
    "lat_sel=lat_array[date_sel]\n",
    "time_sel=time_array[date_sel]\n",
    "lon_sel=lon_array[date_sel]\n",
    "samples=np.array([lat_sel, lon_sel, time_sel]).T\n",
    "noon_time=datetime.datetime.combine(date_of_interest, datetime.time(12,0,0), tzinfo=datetime.timezone.utc) + np.array([datetime.timedelta(hours=h) for h in (-lon_sel / 360 * 24)])\n",
    "time_diff=np.array([t.total_seconds()/3600 for t in (time_sel-noon_time)]) - 1.6\n",
    "\n",
    "low_range=(time_diff < -5.5) & (time_diff > -6.5)\n",
    "low_range_idx=np.arange(0, lat_sel.shape[0])[low_range]\n",
    "max_local_idx=np.argmax(lat_sel[low_range])\n",
    "low_time=time_diff[low_range_idx[max_local_idx]]\n",
    "\n",
    "high_range=(time_diff > 5.5) & (time_diff < 6.5)\n",
    "high_range_idx=np.arange(0, lat_sel.shape[0])[high_range]\n",
    "min_local_idx=np.argmin(lat_sel[high_range])\n",
    "high_time=time_diff[high_range_idx[min_local_idx]]\n",
    "valid_time = (time_diff > low_time) & (time_diff < high_time)\n",
    "f=interpolate.interp1d(lat_sel[valid_time], time_diff[valid_time], kind=\"linear\")\n",
    "\n",
    "np.save(\"../../data/processed/latitude_time_diff_sep_6_2014.npy\", np.array([lat_sel[valid_time], time_diff[valid_time]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab077f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODIS_GAPFILLED_DIR=\"MCD43C4.006_16day/\"\n",
    "PROCESSED_DIR=\"../../data/processed/\"\n",
    "SIF_MODIS_OUT_DIR_V5=\"../../data/processed/SIF_MODIS_16day_v5\"\n",
    "sif_processed_dir = os.path.join(\"../../data/\", \"processed\")\n",
    "fig_dir=\"./figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa93f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(SIF_MODIS_OUT_DIR_V5)\n",
    "for i in range(2000, 2022, 1):\n",
    "    os.mkdir(os.path.join(SIF_MODIS_OUT_DIR_V5, str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773508ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_time_diff=np.load(\"../../data/processed/latitude_time_diff_sep_6_2014.npy\")\n",
    "f=interpolate.interp1d(latitude_time_diff[0], latitude_time_diff[1], kind=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3dc2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_sza_for_fitted_overpass(latitude, date_of_interest):\n",
    "    overpass_time_diff=float(f(latitude))\n",
    "    overpass_time_delta=datetime.timedelta(hours=overpass_time_diff)\n",
    "    cos_sza=np.cos(astral.sun.zenith(astral.LocationInfo(latitude=latitude, longitude=0).observer,\n",
    "                             dateandtime=datetime.datetime.combine(date_of_interest, datetime.time(13,36)) + overpass_time_delta,\n",
    "                             with_refraction = True) / 180 * np.pi)\n",
    "    return cos_sza\n",
    "\n",
    "def compute_daily_sza_for_fitted_overpass(latitude, date_of_interest):\n",
    "    overpass_time_diff=float(f(latitude))\n",
    "    overpass_time_delta=datetime.timedelta(hours=overpass_time_diff)\n",
    "    eval_points=np.arange(-0.5,0.501, 1/(6*24))\n",
    "    daily_sza_points=np.array([np.cos(astral.sun.zenith(astral.LocationInfo(latitude=latitude, longitude=0).observer,\n",
    "                             dateandtime=datetime.datetime.combine(date_of_interest, datetime.time(13,36)) + overpass_time_delta + datetime.timedelta(days=p),\n",
    "                             with_refraction = True) / 180 * np.pi) for p in eval_points])\n",
    "    daily_sza_points[daily_sza_points < 0] = 0\n",
    "    return np.mean(daily_sza_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f576ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0): \n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "# feedforward model construct function\n",
    "def construct_model(input_dim, hidden_dim, n_hidden_layers, drop_out=None):\n",
    "    layers=[]\n",
    "    layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "    layers.append(nn.ReLU())\n",
    "    if drop_out:\n",
    "        layers.append(nn.Dropout(p=0.2))\n",
    "    for i in range(n_hidden_layers - 1):\n",
    "        layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        if drop_out:\n",
    "            layers.append(nn.Dropout(p=drop_out))\n",
    "    layers.append(nn.Linear(hidden_dim, 1))\n",
    "    return nn.Sequential(*layers).to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23568419",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim=64\n",
    "n_hidden_layers=2\n",
    "net= construct_model(3, hidden_dim, n_hidden_layers)\n",
    "model_name=\"layer_2_neuron_64_08-16-2022_16-22-24_lr0.001_batchsize1024\"\n",
    "model_dir=\"./models\"\n",
    "net.load_state_dict(torch.load(os.path.join(model_dir, model_name), map_location=torch.device('cpu')))\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6ed2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_ds = xr.open_dataset(os.path.join(sif_processed_dir, \"train_val.nc\"))\n",
    "XY = np.stack([train_val_ds.Nadir_Reflectance_Band1.values,\n",
    "train_val_ds.Nadir_Reflectance_Band2.values,\n",
    "np.cos(train_val_ds.SZA.values / 180 * np.pi), train_val_ds.SIF_757nm.values]).T\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(XY[:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779338c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_mask=np.load(\"../../data/processed/water_mask.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95beec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sif_prediction_for_a_year(year_list):\n",
    "    if len(os.listdir(os.path.join(SIF_AVHRR_OUT_DIR_V5, year_list[0].split(\"/\")[6]))) < 24:\n",
    "        for file in year_list:\n",
    "            generate_sif_prediction(file)\n",
    "    print(year_list[0].split(\"/\")[6] + \" finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1443fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sif_prediction_modis(file):\n",
    "    ds=xr.open_dataset(file)\n",
    "    valid_flag_modis=(np.invert(np.isnan(ds.red_filled.values[0]))) & (np.invert(np.isnan(ds.nir_filled.values[0])))\n",
    "    red_valid_modis = ds.red_filled.values[0][valid_flag_modis]\n",
    "    nir_valid_modis = ds.nir_filled.values[0][valid_flag_modis]\n",
    "\n",
    "    year=int(file.split(\"/\")[-1].split(\".\")[1][0:4])\n",
    "    year_str=file.split(\"/\")[-1].split(\".\")[1][0:4]\n",
    "    month=int(file.split(\"/\")[-1].split(\".\")[1][4:6])\n",
    "    month_str=file.split(\"/\")[-1].split(\".\")[1][4:6]\n",
    "    period=file.split(\"/\")[-1].split(\".\")[1][6]\n",
    "    if period==\"a\":\n",
    "        day=8\n",
    "        day_str=\"15\"\n",
    "        file_date=np.datetime64(year_str + \"-\" + month_str + \"-\" + day_str)\n",
    "    else:\n",
    "        day=24\n",
    "        file_date=(np.datetime64(year_str + \"-\" + month_str) + np.timedelta64(1, 'M')).astype('datetime64[D]') - np.timedelta64(1, 'D')\n",
    "        \n",
    "    doi=date(year,month,day)\n",
    "    \n",
    "    # compute cos sza\n",
    "    computed_cos_sza=np.full(3600, np.nan)\n",
    "    computed_cos_sza[164:-164]=np.array([compute_cos_sza_for_fitted_overpass(l, doi) for l in ds.latitude.values[164:-164]])\n",
    "    computed_cos_sza_array=np.tile(computed_cos_sza, (7200,1)).T\n",
    "\n",
    "    computed_cos_daily_sza=np.full(3600, np.nan)\n",
    "    computed_cos_daily_sza[164:-164]=np.array([compute_daily_sza_for_fitted_overpass(l, doi) for l in ds.latitude.values[164:-164]])\n",
    "    computed_cos_daily_sza_array=np.tile(computed_cos_daily_sza, (7200,1)).T\n",
    "\n",
    "    computed_cos_sza_valid_modis=computed_cos_sza_array[valid_flag_modis]\n",
    "    computed_cos_daily_sza_valid_modis=computed_cos_daily_sza_array[valid_flag_modis]\n",
    "    data_matrix_modis = np.array([red_valid_modis, nir_valid_modis, computed_cos_sza_valid_modis]).T\n",
    "    \n",
    "    scaled_data_matrix_modis = scaler.transform(data_matrix_modis)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            predicted_modis=net(torch.tensor(scaled_data_matrix_modis).float().to(try_gpu())).cpu().numpy().squeeze()\n",
    "\n",
    "    sif_modis=np.zeros((3600, 7200))\n",
    "    sif_modis[valid_flag_modis]=predicted_modis\n",
    "    sif_modis[np.invert(valid_flag_modis)]=np.nan\n",
    "    sif_modis[water_mask==1] = np.nan\n",
    "\n",
    "    cos_sza_modis=np.zeros((3600, 7200))\n",
    "    cos_sza_modis[valid_flag_modis]=computed_cos_sza_valid_modis\n",
    "    cos_sza_modis[np.invert(valid_flag_modis)]=np.nan\n",
    "    cos_sza_modis[water_mask==1] = np.nan\n",
    "    \n",
    "    cos_daily_sza_modis=np.zeros((3600, 7200))\n",
    "    cos_daily_sza_modis[valid_flag_modis]=computed_cos_daily_sza_valid_modis\n",
    "    cos_daily_sza_modis[np.invert(valid_flag_modis)]=np.nan\n",
    "    cos_daily_sza_modis[water_mask==1] = np.nan\n",
    "\n",
    "\n",
    "    sif_array_modis=xr.DataArray(np.expand_dims(sif_modis, axis=0),\n",
    "                     coords=[[file_date,], ds.latitude, ds.longitude],\n",
    "                     dims=[\"time\", \"latitude\", \"longitude\"])\n",
    "    \n",
    "    sif_daily_array_modis=xr.DataArray(np.expand_dims(sif_modis / cos_sza_modis * cos_daily_sza_modis, axis=0),\n",
    "                     coords=[[file_date,], ds.latitude, ds.longitude],\n",
    "                     dims=[\"time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    cos_sza_array_modis=xr.DataArray(np.expand_dims(cos_sza_modis, axis=0),\n",
    "                     coords=[[file_date,], ds.latitude, ds.longitude],\n",
    "                     dims=[\"time\", \"latitude\", \"longitude\"])\n",
    "    \n",
    "    cos_daily_sza_array_modis=xr.DataArray(np.expand_dims(cos_daily_sza_modis, axis=0),\n",
    "                     coords=[[file_date,], ds.latitude, ds.longitude],\n",
    "                     dims=[\"time\", \"latitude\", \"longitude\"])\n",
    "\n",
    "    modis_sif_ds=xr.Dataset({\"sif_modis_clear_inst\":sif_array_modis,\n",
    "                             \"sif_modis_clear_daily\":sif_daily_array_modis,\n",
    "                             \"cos_sza_modis\":cos_sza_array_modis,\n",
    "                             \"cos_daily_sza_modis\":cos_daily_sza_array_modis})\n",
    "    modis_sif_ds.to_netcdf(os.path.join(SIF_MODIS_OUT_DIR_V5, \"/\".join(file.split(\"/\")[6:])))\n",
    "    modis_sif_ds.close()\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e7c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODIS_FILE_LIST=[]\n",
    "for MODIS_YEAR in sorted(os.listdir(MODIS_GAPFILLED_DIR)):\n",
    "    for MODIS_FILE in sorted(os.listdir(os.path.join(MODIS_GAPFILLED_DIR, MODIS_YEAR))):\n",
    "        file=os.path.join(MODIS_GAPFILLED_DIR, MODIS_YEAR, MODIS_FILE)\n",
    "        MODIS_FILE_LIST.append(file)\n",
    "        \n",
    "MODIS_FILE_YEAR_LIST_AVAILABLE=[]\n",
    "for MODIS_YEAR in sorted(os.listdir(MODIS_GAPFILLED_DIR)):\n",
    "    year_list=[]\n",
    "    for MODIS_FILE in sorted(os.listdir(os.path.join(MODIS_GAPFILLED_DIR, MODIS_YEAR))):\n",
    "        file=os.path.join(MODIS_GAPFILLED_DIR, MODIS_YEAR,MODIS_FILE)\n",
    "        year_list.append(file)\n",
    "    MODIS_FILE_YEAR_LIST_AVAILABLE.append(year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44d62897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_modis_prediction_for_a_year(year_list):\n",
    "    if len(os.listdir(os.path.join(SIF_MODIS_OUT_DIR_V5, year_list[0].split(\"/\")[6]))) < 24:\n",
    "        for file in year_list:\n",
    "            generate_sif_prediction_modis(file)\n",
    "    print(year_list[0].split(\"/\")[6] + \" finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e636c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002 finished!\n",
      "2000 finished!\n",
      "2001 finished!\n",
      "2005 finished!\n",
      "2003 finished!\n",
      "2004 finished!\n",
      "2007 finished!\n",
      "2008 finished!\n",
      "2006 finished!\n",
      "2009 finished!\n",
      "2011 finished!\n",
      "2010 finished!\n",
      "2013 finished!\n",
      "2012 finished!\n",
      "2014 finished!\n",
      "2016 finished!\n",
      "2015 finished!\n",
      "2017 finished!\n",
      "2019 finished!\n",
      "2020 finished!\n",
      "2018 finished!\n",
      "2021 finished!\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(6)\n",
    "jobs = []\n",
    "\n",
    "for year_list in MODIS_FILE_YEAR_LIST_AVAILABLE:\n",
    "    if len(os.listdir(os.path.join(SIF_MODIS_OUT_DIR_V5, year_list[0].split(\"/\")[6]))) < 24:\n",
    "        job = pool.apply_async(generate_modis_prediction_for_a_year,(year_list,))\n",
    "        jobs.append(job)\n",
    "\n",
    "for job in jobs: \n",
    "    job.get()\n",
    "\n",
    "    #now we are done, kill the listener\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
